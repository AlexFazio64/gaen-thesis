{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clean = pd.read_json('---')\n",
    "clean.columns = ['anchor', 'positive', 'original']\n",
    "entries = pd.DataFrame(columns=['anchor', 'positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_all(url='http://localhost:5173/api/data/all'):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "def get_positive(anchor):\n",
    "    url = f\"http://localhost:5173/api/data/{anchor}/positive\"\n",
    "    return get_all(url)\n",
    "\n",
    "def get_rand_negative(anchor):\n",
    "    url = f\"http://localhost:5173/api/data/{anchor}/negative\"\n",
    "    return get_all(url)\n",
    "    \n",
    "def get_adjacent(anchor):\n",
    "    url = f\"http://localhost:5173/api/data/{anchor}/adjacent\"\n",
    "    return get_all(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.DataFrame(get_all(), columns=['anchor'])\n",
    "queries.drop(queries[queries['anchor'].str.contains('/')].index, inplace=True)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in clean.iterrows():\n",
    "    index, data = row\n",
    "    \n",
    "    print(data['anchor'])\n",
    "    try:\n",
    "        adj = pd.DataFrame(get_adjacent(data['original'])).dropna().sample().iloc[0]\n",
    "        positive = pd.DataFrame(get_positive(adj[0])).dropna().sample().iloc[0].loc['document']\n",
    "        entries.loc[len(entries)] = [data['anchor'], data['positive'], positive]\n",
    "    except:\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries.to_json('---', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spoilers = pd.read_json(\"---\")\n",
    "spoilers.columns = [\"text\", \"spoiler\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers[spoilers.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers.drop_duplicates(inplace=True)\n",
    "spoilers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers.sort_values(by='label', inplace=True, ascending=False)\n",
    "spoilers.reset_index(drop=True, inplace=True)\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers[spoilers['text'].str.contains(\"^\\\\s.{,39}:\\\\s\")]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "spoilers['text'] = spoilers['text'].apply(lambda x: re.sub(r'^.{,40}:\\s', '', x))\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace text that starts with a colon and a space with an empty string\n",
    "spoilers['text'] = spoilers['text'].str.replace('\\n', '')\n",
    "spoilers['text'] = spoilers['text'].str.replace('\\\\s+', ' ', regex=True)\n",
    "spoilers['text'] = spoilers['text'].str.replace('\"', '')\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers['text_length'] = spoilers['text'].apply(len)\n",
    "spoilers.sort_values(by='text_length', inplace=True, ascending=True)\n",
    "spoilers['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column types\n",
    "spoilers['text'] = spoilers['text'].astype(str)\n",
    "spoilers['spoiler'] = spoilers['spoiler'].astype(str)\n",
    "spoilers['label'] = spoilers['label'].astype(bool)\n",
    "spoilers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all rows with text length less than 60 and label 1\n",
    "short_spoiler =  spoilers[(spoilers['text_length'] < 35) & (spoilers['label'] == 1)]\n",
    "short_spoiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_no_spoiler = spoilers[(spoilers['text_length'] < 100) & (spoilers['label'] == 0)]\n",
    "short_no_spoiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_spoiler = spoilers[(spoilers['text_length'] > 150) & (spoilers['label'] == 1)]\n",
    "long_spoiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers['spoiler'] = spoilers['spoiler'].str.replace('\\n', '')\n",
    "spoilers['spoiler'] = spoilers['spoiler'].str.replace('\\\\s+', ' ', regex=True)\n",
    "spoilers['spoiler'] = spoilers['spoiler'].str.replace('\"', '')\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of spoiler length to text length\n",
    "spoilers['ratio'] = spoilers['spoiler'].apply(len) / spoilers['text_length']\n",
    "spoilers.sort_values(by='ratio', inplace=True, ascending=False)\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers[spoilers['label']==True]['ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop text_length and label the columns\n",
    "spoilers.drop(columns=['text_length', 'label'], inplace=True)\n",
    "spoilers.columns = ['text', 'spoiler', 'spoiler/text']\n",
    "spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers.drop_duplicates(inplace=True)\n",
    "spoilers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoilers.to_json('---', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'id1': 'Koyomi Araragi', 'id2': 'Hitagi Senjougahara'}\n",
    "response = requests.post('http://localhost:5173/api/similarity', json=params)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = queries['anchor'].to_list()\n",
    "responses = []\n",
    "\n",
    "for k1 in keys:\n",
    "  pairs = []\n",
    "  for k2 in keys:\n",
    "      if k1 != k2:\n",
    "        pairs.append((k1, k2))\n",
    "  response = requests.post('http://localhost:5173/api/similarity', json=pairs)\n",
    "  for r in response.json():\n",
    "    responses.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://localhost:5173/api/data/filter', json={ \"filters\": ['Characters', 'Novels', 'Oddities', 'Oddity Characters', 'Oddity Inflicted Characters', 'Oddity Specialists', 'Oddity Type', 'Terminology', 'Vampire Hunter', 'Vampires']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]\n",
    "id = nodes[0].get(\"id\")\n",
    "descr = nodes[0].get(\"Description\")\n",
    "\n",
    "print(id)\n",
    "print(descr)\n",
    "\n",
    "for _ in nodes[0].keys():\n",
    "    if type(nodes[0][_]) == dict:\n",
    "        for __ in nodes[0][_].keys():\n",
    "            if(__ == \"text\") and len(nodes[0][_][__]) > 0:\n",
    "                print(f\"{_} -> {__}: {nodes[0][_][__]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for node in nodes:\n",
    "    id = node.get(\"id\")\n",
    "    descr = node.get(\"Description\")\n",
    "\n",
    "    dataset.append({\n",
    "        \"anchor\": id,\n",
    "        \"positive\": descr\n",
    "    })\n",
    "\n",
    "    for _ in node.keys():\n",
    "        if type(node[_]).__name__ == 'dict':\n",
    "            for __ in node[_].keys():\n",
    "                if(__ == \"text\") and len(node[_][__]) > 0:\n",
    "                    dataset.append({\n",
    "                        \"anchor\": id,\n",
    "                        \"positive\": node[_][__]\n",
    "                    })\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['anchor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def has_html(text):\n",
    "    return bool(re.search(r'<[^>]+>', text))\n",
    "\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"positive\"] = dataset[\"positive\"].str.replace(r'\\(.*?\\)', '', regex=True)\n",
    "dataset[\"positive\"] = dataset[\"positive\"].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "dataset[\"positive\"] = dataset[\"positive\"].str.replace(r'\\n', ' ', regex=True)\n",
    "dataset[\"positive\"] = dataset[\"positive\"].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['positive'] = dataset['positive'].str.strip()\n",
    "dataset['positive'] = dataset['positive'].str.lower()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy()\n",
    "df[\"has_html\"] = df[\"positive\"].apply(has_html)\n",
    "df[df[\"has_html\"] == True]\n",
    "\n",
    "dataset.drop(df[df[\"has_html\"] == True].index, inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_json('---', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('---')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = dataset[dataset['positive'].str.contains(r'^\".*\"')].copy()\n",
    "quotes.drop(quotes[quotes['anchor'].str.contains('monogatari')].index, inplace=True)\n",
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['positive'] = quotes['positive'].str.split(r'\"\\s\"')\n",
    "quotes = quotes.explode('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['positive'] = quotes['positive'].str.replace(r'\"', '', regex=True)\n",
    "quotes['positive'] = quotes['positive'].str.strip()\n",
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_quotes = dataset.drop(quotes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split positive by phrases\n",
    "no_quotes['positive'] = no_quotes['positive'].str.split(r'\\.\\s+')\n",
    "no_quotes = no_quotes.explode('positive')\n",
    "no_quotes.reset_index(drop=True, inplace=True)\n",
    "no_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = no_quotes.query('positive.str.len() < 35')\n",
    "no_quotes.drop(drop.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([no_quotes, quotes])\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(by='anchor', inplace=True)\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_json('---', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('---')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mutate_text(text, temp=0.4):\n",
    "  \"\"\"\n",
    "  has a chance of mutating the text possible mutations are:\n",
    "    - removing 'monogatari' from the text if present\n",
    "    - dropping words if the text has more than 3 words\n",
    "    - order reversal of words in the text\n",
    "\n",
    "  Args:\n",
    "      text (str): 'anchor' column value\n",
    "      temp (float): temperature value for mutation\n",
    "  \"\"\"\n",
    "  \n",
    "  # check if the text contains 'monogatari'\n",
    "  if 'monogatari' in text:\n",
    "    if(random.random() > temp):\n",
    "      return text\n",
    "    text = text.replace('monogatari', '')\n",
    "    return text.strip()\n",
    "    \n",
    "  if(random.random() > temp):\n",
    "    return text\n",
    "  \n",
    "  # check if text has more than 1 word\n",
    "  words = text.split()\n",
    "  if len(words) == 2:\n",
    "    choice = random.choice([\"drop\", \"reverse\"])\n",
    "    if choice == \"drop\":\n",
    "      text = random.choice(words)\n",
    "      return text\n",
    "    else:\n",
    "      text = ' '.join(words[::-1])\n",
    "      return text\n",
    "  elif len(words) > 2:\n",
    "    text = random.choice(words)\n",
    "    \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def introduce_typo(word):\n",
    "    typo_type = random.choice([\"substitution\", \"insertion\", \"deletion\", \"transposition\"])\n",
    "    idx = random.randint(0, len(word) - 1)\n",
    "\n",
    "    if typo_type == \"substitution\":\n",
    "        word = word[:idx] + random.choice(\"abcdefghijklmnopqrstuvwxyz\") + word[idx+1:]\n",
    "\n",
    "    elif typo_type == \"insertion\":\n",
    "        word = word[:idx] + random.choice(\"abcdefghijklmnopqrstuvwxyz\") + word[idx:]\n",
    "\n",
    "    elif typo_type == \"deletion\" and len(word) > 1:\n",
    "        word = word[:idx] + word[idx+1:]\n",
    "\n",
    "    elif typo_type == \"transposition\" and len(word) > 1:\n",
    "        if idx < len(word) - 1:\n",
    "            word = word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
    "\n",
    "    return word\n",
    "\n",
    "query = \"Monogatari series\"\n",
    "for _ in range(5):\n",
    "    print(introduce_typo(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated = dataset.copy()\n",
    "entries = pd.DataFrame(columns=['anchor', 'positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in mutated.iterrows():\n",
    "    index, data = row\n",
    "    \n",
    "    print(data['anchor'])\n",
    "    try:\n",
    "        adj = pd.DataFrame(get_adjacent(data['anchor'])).dropna().sample().iloc[0]\n",
    "        positive = pd.DataFrame(get_positive(adj[0])).dropna().sample().iloc[0].loc['document']\n",
    "        entries.loc[len(entries)] = [data['anchor'], data['positive'], positive]\n",
    "    except Exception as e:\n",
    "        print(f'\\tERROR -> {e}')\n",
    "        continue\n",
    "\n",
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = entries.copy()\n",
    "split['negative'] = split['negative'].apply(lambda x: x.split(\". \")[0])\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated = split.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated['anchor'] = dataset['anchor'].apply(lambda x: mutate_text(x,.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated.dropna(inplace=True)\n",
    "mutated['anchor'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [...]\n",
    "\n",
    "mutated.drop(mutated[mutated['anchor'].isin(drop)].index, inplace=True)\n",
    "mutated.reset_index(drop=True, inplace=True)\n",
    "mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated[mutated['anchor'].str.contains(r\"'s$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated.drop(mutated[mutated['anchor'].str.contains(r\"'s$\")].index, inplace=True)\n",
    "mutated.drop(mutated[mutated['positive'].str.contains(r'[ぁ-んァ-ン一-龯]+')].index, inplace=True)\n",
    "mutated.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated.to_json('---', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anchor'].sample(5).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in df['anchor'].sample(5).to_list():\n",
    "    print(f'{_} -> {mutate_text(_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy()\n",
    "df['anchor'] = df['anchor'].str.replace(r'-', '', regex=True)\n",
    "df['anchor'] = df['anchor'].str.replace(r'\\'s', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = df['anchor'].str.contains(r'\\s')\n",
    "spaces = df[spaces]\n",
    "spaces['anchor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the anchor into words\n",
    "spaces.loc[:, 'anchor'] = spaces['anchor'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = spaces.explode('anchor').reset_index(drop=True)\n",
    "exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = df['anchor'].str.contains(r'\\s')\n",
    "inverted = df[inverted]\n",
    "\n",
    "# split the anchor into words and reverse the order\n",
    "inverted.loc[:, 'anchor'] = inverted['anchor'].str.split(' ')\n",
    "inverted.loc[:, 'anchor'] = inverted['anchor'].apply(lambda x: x[::-1])\n",
    "inverted.loc[:, 'anchor'] = inverted['anchor'].apply(lambda x: ' '.join(x))\n",
    "inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono = df.query('anchor.str.contains(\"monogatari\") == True')\n",
    "mono['anchor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono.loc[:, 'anchor'] = mono['anchor'].str.replace(r'monogatari', '', regex=True)\n",
    "mono.loc[:, 'anchor'] = mono['anchor'].str.strip()\n",
    "mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([exploded, inverted, mono, dataset])\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.drop_duplicates(inplace=True)\n",
    "merge.reset_index(drop=True, inplace=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find positive phrases that contain japanese characters\n",
    "japanese = merge[merge['positive'].str.contains(r'[ぁ-んァ-ン一-龯]+')]\n",
    "merge.drop(japanese.index, inplace=True)\n",
    "merge.reset_index(drop=True, inplace=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguations = requests.post('http://localhost:5173/api/data/filter', json={\"filters\": [\"disambiguations\"]}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = []\n",
    "for dis in disambiguations:\n",
    "    remove.append(dis.get(\"id\"))\n",
    "    \n",
    "remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove disambiguations\n",
    "merge.drop(merge[merge['anchor'].isin(remove)].index, inplace=True)\n",
    "merge.reset_index(drop=True, inplace=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge['anchor'].sort_values(key=lambda x: x.str.len()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [...]\n",
    "\n",
    "merge.drop(merge[merge['anchor'].isin(drop)].index, inplace=True)\n",
    "merge.reset_index(drop=True, inplace=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.duplicated(subset=['positive']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.drop_duplicates(subset=['positive'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge['anchor'].sort_values(key=lambda x: x.str.len()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.to_json('---', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
