@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention is all you need},
  year      = {2017}
}

@article{mikolov2013efficient,
  author  = {Mikolov, Tomas},
  journal = {arXiv preprint arXiv:1301.3781},
  title   = {Efficient estimation of word representations in vector space},
  volume  = {3781},
  year    = {2013}
}

@misc{google2017transformer,
  author       = {{Google Research}},
  howpublished = {\url{https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/}},
  title        = {Transformer: A Novel Neural Network Architecture for Language Understanding},
  year         = {2017}
}
@misc{olegborisovembeddingspace,
  author       = {{Oleg Borisov}},
  howpublished = {\url{https://medium.com/data-science/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba/}},
  title        = {Embedding Space},
  year         = {2020}
}