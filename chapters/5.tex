\chapter{Valutazione}
\label{ch:valutazione}

\section{Introduzione}
\label{sec:introduzione}
In questo capitolo, verranno presentati i risultati
ottenuti durante la fase di valutazione del nostro sistema.

Iniziando con una panoramica dei dati utilizzati per
fine-tuning e valutazione del modello, seguita da una
descrizione delle metriche di valutazione impiegate.

Successivamente, presenteremo i risultati ottenuti,
analizzando le prestazioni del nostro sistema in termini di
accuratezza e capacità di rilevamento degli spoiler.

Verranno discusse le implicazioni dei risultati e le
possibili direzioni future per il miglioramento del
sistema.

\subsection{Specifiche tecniche}
\label{sec:specs}
La configurazione del sistema è stata eseguita su un
computer con le seguenti specifiche:

\begin{itemize}
  \item CPU: AMD Ryzen 7 5800H (8 core, 16 thread)
  \item GPU: NVIDIA GeForce RTX 3070 Laptop (8 GB)
  \item RAM: 16 GB
  \item Sistema operativo: Windows 10 22H2
  \item Versione di Python: 3.12.9
\end{itemize}

\section{Dataset e metodologia}
\label{sec:dataset_eval}

Per la valutazione del nostro sistema, abbiamo utilizzato 2
strategie: \textbf{pair} e \textbf{triplet}.
Il dataset \textbf{pair} è derivato da \textbf{triplet}
escludendo la colonna \textit{negative}.
Questo ha permesso di valutare il sistema in diverse
configurazioni.

Il codice per la generazione degli split e il training è
identico per entrambe le strategie, con l'unica differenza
che per \textbf{pair}, il dataset è stato ridotto a 2
colonne: \textit{positive} e \textit{negative}, adattando
la funzione di loss di conseguenza.
Il seed utilizzato per la generazione degli split è
\texttt{298}\cite{nisemonogatari_ep1}, garantendo la
riproducibilità dei risultati.

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} r}
    \toprule
    \textbf{Parametro} & \textbf{Valore}       \\
    \midrule
    Dataset            & 4067 righe            \\
    Split              & 80\% train, 20\% test \\
    Epoche             & 4                     \\
    Loss (pair)        & CosineSimilarityLoss  \\
    Loss (triplet)     & TripletLoss           \\
    \bottomrule
  \end{tabularx}
  \caption{Panoramica dei parametri utilizzati per il fine-tuning e la valutazione del modello.}
  \label{tab:dataset_eval}
\end{table}

\section{Embedders}
\label{sec:embedders_eval}
Durante la creazione del sistema, si è deciso di
valutare due modelli pre-addestrati diversi tra loro per generare gli embedding, quali:

\begin{itemize}
  \item \textbf{all-MiniLM-L6-v2}
  \item \textbf{all-mpnet-base-v2}
\end{itemize}

Nessuno dei modelli è stato addestrato su dati specifici al
dominio degli spoiler o sui dati utilizzati per il
fine-tuning.
Come mostrano i risultati di seguito, nessuno dei modelli
soddisfaceva i requisiti per il task dato il dominio
specifico, ma con un fine-tuning adeguato, sono stati in
grado di generare embeddings soddisfacenti.

Entrambi i modelli sono disponibili su HuggingFace,
installando la libreria \texttt{sentence-transformers}.
I modelli scelti sono comunemente utilizzati per la
creazione di embeddings per frasi o paragrafi brevi,
rendendoli adatti al nostro task.
Il codice per il fine-tuning e la valutazione è stato
scritto in Python ed è disponibile su
GitHub\cite{af64_spoiler_filter}.

\newpage
\subsection{Risultati}
\label{sec:risultati}
\textbf{Nota:} I migliori risultati sono segnalati con un asterisco
(\textbf{*})

\subsubsection{Pair}
\label{sec:pair}
\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} r}
    \toprule
    Modello                     & {Similarità positiva media} \\
    \midrule
    \multicolumn{2}{c}{\textbf{Pre-addestramento}}            \\
    \midrule
    all-mpnet-base-v2           & 0.1554                      \\
    \textbf{all-MiniLM-L6-v2}*  & \textbf{0.1688}             \\
    \midrule
    \multicolumn{2}{c}{\textbf{Fine-tuned}}                   \\
    \midrule
    \textbf{all-mpnet-base-v2}* & \textbf{0.9975}             \\
    all-MiniLM-L6-v2            & 0.9938                      \\
    \bottomrule
  \end{tabularx}
  \caption{Risultati della valutazione dei modelli di embedding (Pair)}
  \label{tab:embedding_pair}
\end{table}

\subsubsection{Triplet}
\label{sec:triplet}
\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{\fill}} r}
    \toprule
    Modello                     & {Similarità pos. media} & {Similarità neg. media} \\
    \midrule
    \multicolumn{3}{c}{\textbf{Pre-addestramento}}                                  \\
    \midrule
    all-mpnet-base-v2           & 0.1554                  & 0.2001                  \\
    \textbf{all-MiniLM-L6-v2}*  & \textbf{0.1688}         & \textbf{0.2113}         \\
    \midrule
    \multicolumn{3}{c}{\textbf{Fine-tuned}}                                         \\
    \midrule
    \textbf{all-mpnet-base-v2}* & \textbf{0.6358}         & \textbf{-0.8118}        \\
    all-MiniLM-L6-v2            & 0.6274                  & -0.7221                 \\
    \bottomrule
  \end{tabularx}
  \caption{Risultati della valutazione dei modelli di embedding (Triplet)}
  \label{tab:embedding_triplet}
\end{table}

Notiamo che i risultati pre-addestramento sono molto bassi,
e simili tra loro, il che indica che i modelli non sanno
categorizzare bene i dati in input.
Tuttavia, dopo il fine-tuning, i risultati sono
significativamente migliorati, con all-mpnet-base-v2 che
ottiene i migliori risultati in entrambe le configurazioni.

\noindent\textbf{Nota:}
I risultati nella configurazione \textbf{pair} sono
migliori rispetto a quelli nella configurazione
\textbf{triplet}, ma ciò indica solo che i modelli hanno
appreso a generare embedding più simili tra loro,
\textbf{non necessariamente più accurati}.

\subsection{Considerazioni}
\label{sec:considerazioni}

In definitiva, i risultati mostrano che il fine-tuning
porta a un miglioramento significativo delle prestazioni,
specialmente nella configurazione \textbf{triplet}, dove i
modelli sono stati in grado di generare embedding con una
similarità positiva superiore a $0.64$ e una similarità
negativa inferiore a $-0.81$.
Ciò indica che i modelli sono stati in grado di raggruppare
correttamente i dati simili e separare quelli dissimili,
dimostrando la loro efficacia in questo task dopo un
adeguato fine-tuning.

Nononstante all-mpnet-base-v2 abbia ottenuto, di poco,
risultati migliori rispetto a all-MiniLM-L6-v2,
quest'ultimo ha dimostrato di essere più veloce e più
leggero, rendendolo la scelta preferita per questo lavoro.
Inoltre, all-MiniLM-L6-v2 ha mostrato prestazioni migliori
in fase di fine-tuning.

\subsection{Prestazioni in fase di fine-tuning}
\label{sec:prestazioni-fine-tuning}

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} c @{\extracolsep{\fill}} c @{\extracolsep{\fill}} r}
    \toprule
    Modello                    & {Tempo (min:sec)} & Step & {Perdita} \\
    \midrule
    all-mpnet-base-v2          & 03:26             & 500  & 3.8035    \\
    \textbf{all-MiniLM-L6-v2}* & \textbf{00:53}    & 500  & 4.0104    \\
    \bottomrule
  \end{tabularx}
  \caption{Tempi e perdite del fine-tuning (Triplet)}
  \label{tab:finetuning_triplets}
\end{table}

\begin{table}[H]
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} c @{\extracolsep{\fill}} c @{\extracolsep{\fill}} r}
    \toprule
    Modello                   & {Tempo (min:sec)} & Step & {Perdita} \\
    \midrule
    all-mpnet-base-v2         & 02:10             & 500  & 0.0183    \\
    \textbf{all-MiniLM-L6-v2} & \textbf{00:37}    & 500  & 0.0263    \\
    \bottomrule
  \end{tabularx}
  \caption{Tempi e perdite del fine-tuning (Pair)}
  \label{tab:finetuning_pairs}
\end{table}

Le prestazioni di all-MiniLM-L6-v2 sono notevolemente
migliori rispetto a quelle di all-mpnet-base-v2 in termini
di tempo.
Il tempo di inferenza è un aspetto critico per
l'implementazione di modelli che devono essere utilizzati
in locale, e questo modello ha dimostrato di essere la
scelta più adatta per il nostro scopo.

\section{LLM}
\label{sec:llm_eval}

Per la valutazione del modello LLM, è stato scelto
\textbf{Gemma3}.

\subsection{Gemma3}
\label{sec:gemma3}
Gemma3 è un modello LLM open-weight sviluppato da Google,
progettato per essere altamente efficiente e performante.
Gemma3 ha capacità multimodali, il che significa che può
elaborare testo e immagini.

Data la sua architettura avanzata derivante da Gemini, lo
stato dell'arte dei modelli di Google, Gemma3 è in grado di
comprendere e generare contenuti con performance
competitive rispetto a modelli di dimensioni molto
maggiori\cite{gemma_2025}.

Per questo progetto, non c'è stato bisogno di utilizzare le
capacità multimodali, bensì ci siamo concentrati sulla
generazione di testo.

Si è deciso di utilizzare Gemma3 per la sua capacità di
generare testo di alta qualità e la sua architettura
leggera che consente un'implementazione rapida ed
efficiente.

A causa di limitazioni hardware, non è stato possibile
scaricare il modello completo, quindi è stato scelto di
utilizzare versioni del modello con dimensioni diverse.
Gemma3 è disponibile in diverse dimensioni:

\begin{itemize}
  \item \textbf{Gemma3-1B}: 1 miliardo di parametri
  \item \textbf{Gemma3-4B}: 4 miliardi di parametri
  \item \textbf{Gemma3-12B}: 12 miliardi di parametri
  \item \textbf{Gemma3-27B}: 27 miliardi di parametri
\end{itemize}

La versione 1B non possiede la capacità multimodali, mentre
la versione 12B ha una dimensione $>8GB$, per cui non è
stato possibile utilizzare appieno il modello sfruttando
l'accelerazione GPU.

\subsection{Risultati LLM}
\label{sec:llm_results}

Per valutare le prestazioni dei diversi LLM, è stato
utilizzato un dataset diverso rispetto a quello utilizzato
per il fine-tuning.

Il dataset è composto da 100 frasi, ognuna delle quali è
stata generata da un utente diverso.
Le frasi possono contenere spoiler o meno, e in caso di
spoiler, viene marcato l'estratto che contiene lo spoiler.

Il dataset è stato procurato tramite scraping del sito
\textbf{TVTropes}, un sito dedicato alla discussione di
opere di narrativa e alla loro analisi.

Il dataset è stato bilanciato in modo da avere un numero
uguale di frasi con e senza spoiler.
Le frasi sono state estratte in modo casuale dalla comunità
dedicata all'opera \textbf{Monogatari Series}, che è anche
l'oggetto del dataset utilizzato per il fine-tuning del
modello di embedding.

Si è scelto di mantenere il preprocessamento del dataset al
minimo, poiché le frasi sono state raccolte direttamente
dal sito web.
Questo approccio permette di preservare la qualità naturale
del linguaggio utilizzato dagli utenti reali.
Il dataset risultante rappresenta un campione autentico di
espressioni, caratterizzato da varietà di stili e
dall'assenza di schemi predefiniti.

Questa eterogeneità pone una sfida significativa per il
modello, che non è stato addestrato su dati di questa
natura e si trova ad affrontare un tipo di input inedito.

La Tabella \ref{tab:risultati-llm} riassume i risultati
ottenuti dai vari modelli.

\begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{l @{\extracolsep{\fill}} cccc}
    \toprule
    Modello              & Accuracy      & Precision      & Recall       & F1-score       \\
    \midrule
    Gemma3:1b            & 0.51          & 0.505          & \textbf{1.0} & 0.671          \\
    Gemma3:4b            & 0.58          & 0.543          & \textbf{1.0} & 0.704          \\
    \textbf{Gemma3:12b}* & \textbf{0.66} & \textbf{0.603} & 0.94         & \textbf{0.734} \\
    Gemini 2.0-flash     & 0.59          & 0.552          & 0.96         & 0.701          \\
    \bottomrule
  \end{tabularx}
  \caption{Risultati delle prestazioni dei modelli.}
  \label{tab:risultati-llm}
\end{table}

\section{Analisi dei risultati}
\label{sec:analisi-risultati}

I risultati ottenuti mostrano che la dimensione del modello
ha un impatto notevole sulle prestazioni.
Si osserva che Gemma3-12B ha ottenuto risultati migliori in
termini di accuratezza, precisione e F1-score, rispetto
invece a modelli più piccoli, come Gemma3-1B che mostra
risultati simili ad un lancio di moneta, con un'accuratezza
intorno al 50\%.
Ricordiamo che:

\begin{itemize}
  \item Accuracy: Misura la proporzione di previsioni corrette
        sul totale delle previsioni.
  \item Precision: Misura quanto sono affidabili le previsioni
        positive del modello.
  \item Recall: Misura la capacità del modello di trovare tutti i
        casi positivi.
  \item F1-score: Fornisce un equilibrio tra precisione e recall,
        utile quando si desidera considerare entrambi gli aspetti.
\end{itemize}

Dato il trend osservato, si è deciso di introdurre un
ulteriore modello, \textbf{Gemini 2.0-flash}, top di gamma
di Google, per confrontare le prestazioni con quelle di
Gemma3-12B.

Utilizzando la stessa metodologia di valutazione (con la
differenza che Gemini è disponibile solamente attraverso
API di Google), Gemini 2.0-flash ha ottenuto risultati
inferiori rispetto a Gemma3-12B, nonostante Gemini
2.0-flash sia un modello più grande e avanzato.

I valori sono stati calcolati tramite la libreria
\texttt{sklearn}, e si basano sulle previsioni del modello
sui dati di test.

Importante notare che i risultati sono stati ottenuti su un
task di classificazione binaria, ovvero rilevare se una
frase contiene uno spoiler o meno, tuttavia, il prompt del
sistema richiedeva \textbf{anche} di evidenziare lo
spoiler, il che potrebbe aver influenzato le prestazioni
dei modelli.
\\

\noindent
Sarebbe interessante esplorare ulteriormente questo aspetto
per capire se i modelli sono in grado di estrarre correttamente
lo spoiler o soffrano di ``allucinazioni'', ma questo
richiederebbe un'analisi più approfondita e ``soggettiva''
dei risultati ottenuti.
Per correttezza, si è preferito non includere questo tipo
analisi in modo da non influenzare i risultati finali.

\section{Conclusioni}
\label{sec:conclusioni}

In questo capitolo, sono stati presentati i risultati
ottenuti durante la fase di valutazione del sistema di
rilevamento degli spoiler.
Sono state discusse le prestazioni dei modelli di embedding
ed LLM, analizzando le loro capacità di categorizzare
correttamente i dati in input e di generare embeddings
ottimali.

L'analisi ha incluso una valutazione approfondita delle
prestazioni dei modelli durante la fase di fine-tuning,
sottolineando come questo processo sia cruciale per
ottimizzare l'efficacia dei modelli stessi.
Inoltre, è stato effettuato un confronto dettagliato tra i
modelli Gemma3 e Gemini 2.0-flash, evidenziando le loro
peculiarità e capacità nel rilevare spoiler.
Questo confronto ha tenuto conto delle diverse dimensioni e
architetture dei modelli, nonché della complessità
intrinseca del compito di rilevamento degli spoiler.
